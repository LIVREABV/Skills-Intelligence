{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32f782a-f1ed-4227-af3b-4784740635e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.robotparser\n",
    "\n",
    "robots_url = \"https://rabobank.jobs/robots.txt\"\n",
    "target_url = \"https://rabobank.jobs/en/jobs/\"\n",
    "\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(robots_url)\n",
    "rp.read()\n",
    "\n",
    "print(\"robots.txt URL:\", robots_url)\n",
    "print(\"Allowed to fetch job detail path?\", rp.can_fetch(\"*\", target_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f765155-4440-4d74-91d5-8889f97f5565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.robotparser\n",
    "\n",
    "robots_url = \"https://werkenbij.devolksbank.nl/robots.txt\"\n",
    "target_url = \"https://werkenbij.devolksbank.nl/nl/nl/search-results\"\n",
    "\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(robots_url)\n",
    "rp.read()\n",
    "\n",
    "print(\"robots.txt URL:\", robots_url)\n",
    "print(\"Target URL:\", target_url)\n",
    "print(\"Allowed?\", rp.can_fetch(\"*\", target_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98dd88ca-160b-4fbe-866e-5fe528371ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook — Unified Scraper (ABN AMRO + ING + De Volksbank)\n",
    "# Filters: Data Analyst / BI Analyst\n",
    "# Output schema (same as ABN AMRO): vacancy_id, title, location_guess, url, sections_text, description_text, date_taken_utc\n",
    "# Saves:\n",
    "# 1) per-source CSV: abnamroYYYYMMDD.csv, ingYYYYMMDD.csv, devolksbankYYYYMMDD.csv\n",
    "# 2) combined CSV: nl_banks_data_or_bi_analyst_YYYYMMDD.csv\n",
    "# Volume target:\n",
    "# /Volumes/skills_intelligence/00_job_posting_landing_zone/financial_sector/data_or_bi_analyst/\n",
    "\n",
    "%pip install -q requests beautifulsoup4 pandas lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c510be4b-aabb-4654-b1bb-6872c81013cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff489c5-4f1a-4e9d-9246-10ed66b96207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import html as ihtml\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE = \"https://werkenbij.devolksbank.nl\"\n",
    "UA = \"MarkusJobScraper/1.0 (+contact: youremail@example.com)\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": UA,\n",
    "    \"Accept-Language\": \"nl-NL,nl;q=0.9,en;q=0.8\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "# We'll scan numeric job IDs like: /nl/nl/job/6217/Some-Title\n",
    "# Pick a reasonable range; adjust if you want broader coverage.\n",
    "START_ID = 6000\n",
    "END_ID   = 8500\n",
    "\n",
    "SLEEP_SEC = 0.35\n",
    "MAX_CONSECUTIVE_MISSES = 200   # stop early when we're likely outside active ID space\n",
    "\n",
    "# Filter: Data Analyst / BI Analyst (EN + NL)\n",
    "KEYWORD_PATTERNS = [\n",
    "    r\"\\bdata analyst\\b\",\n",
    "    r\"\\bsenior data analyst\\b\",\n",
    "    r\"\\bbi analyst\\b\",\n",
    "    r\"\\bbusiness intelligence analyst\\b\",\n",
    "    r\"\\bdata analist\\b\",\n",
    "    r\"\\bsenior data analist\\b\",\n",
    "    r\"\\bbi analist\\b\",\n",
    "    r\"\\bbusiness intelligence analist\\b\",\n",
    "]\n",
    "\n",
    "# Output folder (Unity Catalog Volume)\n",
    "VOLUME_DIR = \"/Volumes/skills_intelligence/00_job_posting_landing_zone/financial_sector/data_or_bi_analyst\"\n",
    "FILE_DATE = datetime.now(timezone.utc).strftime(\"%Y%m%d\")\n",
    "OUT_CSV_VOLUME = f\"{VOLUME_DIR}/devolksbank{FILE_DATE}.csv\"\n",
    "\n",
    "print(\"CSV target:\", OUT_CSV_VOLUME)\n",
    "\n",
    "# =========================\n",
    "# SESSION\n",
    "# =========================\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def is_relevant(title: str, description: str) -> bool:\n",
    "    #text = f\"{title} {description}\"\n",
    "    text = f\"{title}\"\n",
    "    return any(re.search(p, text, flags=re.I) for p in KEYWORD_PATTERNS)\n",
    "\n",
    "def strip_html_to_text(html_str: str) -> str:\n",
    "    \"\"\"Convert HTML fragments to plain text safely.\"\"\"\n",
    "    if not html_str:\n",
    "        return \"\"\n",
    "    # unescape entities first\n",
    "    html_str = ihtml.unescape(html_str)\n",
    "    soup = BeautifulSoup(html_str, \"lxml\")\n",
    "    return norm(soup.get_text(\" \"))\n",
    "\n",
    "def get_jobposting_ldjson(soup: BeautifulSoup) -> dict | None:\n",
    "    \"\"\"\n",
    "    Find JobPosting JSON-LD (best for JS-heavy pages).\n",
    "    Returns the JobPosting dict if present.\n",
    "    \"\"\"\n",
    "    scripts = soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"})\n",
    "    for sc in scripts:\n",
    "        raw = sc.string or sc.get_text() or \"\"\n",
    "        raw = raw.strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except Exception:\n",
    "            # Sometimes there are multiple JSON blocks or invalid JSON; skip safely\n",
    "            continue\n",
    "\n",
    "        # JSON-LD can be dict or list\n",
    "        candidates = data if isinstance(data, list) else [data]\n",
    "        for item in candidates:\n",
    "            if isinstance(item, dict) and item.get(\"@type\") == \"JobPosting\":\n",
    "                return item\n",
    "    return None\n",
    "\n",
    "def location_from_jobposting(job: dict) -> str:\n",
    "    \"\"\"\n",
    "    Extract location guess from JobPosting -> jobLocation.\n",
    "    \"\"\"\n",
    "    loc = \"\"\n",
    "    jl = job.get(\"jobLocation\")\n",
    "    if isinstance(jl, list) and jl:\n",
    "        jl = jl[0]\n",
    "    if isinstance(jl, dict):\n",
    "        addr = jl.get(\"address\")\n",
    "        if isinstance(addr, dict):\n",
    "            parts = []\n",
    "            # Common fields\n",
    "            for k in [\"streetAddress\", \"addressLocality\", \"addressRegion\", \"postalCode\", \"addressCountry\"]:\n",
    "                v = addr.get(k)\n",
    "                if v:\n",
    "                    parts.append(str(v))\n",
    "            loc = \", \".join(parts)\n",
    "    return norm(loc)\n",
    "\n",
    "def fetch_vacancy(vac_id: int) -> dict | None:\n",
    "    \"\"\"\n",
    "    Return dict schema IDENTICAL to ABN/ING:\n",
    "    vacancy_id, title, location_guess, url, sections_text, description_text, date_taken_utc\n",
    "\n",
    "    We request a canonical-ish URL and allow redirects; many job sites redirect to /job/<id>/<slug>.\n",
    "    \"\"\"\n",
    "    date_taken_utc = datetime.now(timezone.utc)\n",
    "\n",
    "    # Try with trailing slash to maximize redirect success\n",
    "    url = f\"{BASE}/nl/nl/job/{vac_id}/\"\n",
    "    r = session.get(url, allow_redirects=True, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # Prefer JSON-LD JobPosting (stable for JS-heavy)\n",
    "    job = get_jobposting_ldjson(soup)\n",
    "\n",
    "    title = \"\"\n",
    "    description_text = \"\"\n",
    "    location_guess = \"\"\n",
    "\n",
    "    if job:\n",
    "        title = norm(job.get(\"title\", \"\"))\n",
    "        # description in JobPosting is often HTML\n",
    "        description_text = strip_html_to_text(job.get(\"description\", \"\"))\n",
    "        location_guess = location_from_jobposting(job)\n",
    "\n",
    "    # Fallback if JSON-LD absent: use visible text\n",
    "    if not title:\n",
    "        h1 = soup.find(\"h1\")\n",
    "        title = norm(h1.get_text(\" \")) if h1 else \"\"\n",
    "    if not description_text:\n",
    "        main = soup.find(\"main\") or soup\n",
    "        description_text = norm(main.get_text(\" \"))\n",
    "\n",
    "    # If still no title, treat as not a real vacancy page\n",
    "    if not title:\n",
    "        return None\n",
    "\n",
    "    # sections_text: best-effort split by headings if present; otherwise empty\n",
    "    sections = []\n",
    "    main = soup.find(\"main\") or soup\n",
    "    for hdr in main.find_all([\"h2\", \"h3\"]):\n",
    "        header = norm(hdr.get_text(\" \"))\n",
    "        if not header:\n",
    "            continue\n",
    "        parts = []\n",
    "        for sib in hdr.find_all_next():\n",
    "            if sib.name in [\"h2\", \"h3\"]:\n",
    "                break\n",
    "            if sib.name in [\"p\", \"ul\", \"ol\"]:\n",
    "                parts.append(norm(sib.get_text(\" \")))\n",
    "        content = norm(\" \".join([p for p in parts if p]))\n",
    "        if len(content) >= 80:\n",
    "            sections.append(f\"{header}: {content}\")\n",
    "    sections_text = \"\\n\\n\".join(sections)\n",
    "\n",
    "    # If location is still empty, do quick heuristic\n",
    "    if not location_guess:\n",
    "        txt = description_text[:1500].lower()\n",
    "        for city in [\"utrecht\", \"amsterdam\", \"den haag\", \"the hague\", \"rotterdam\", \"eindhoven\", \"hybride\", \"hybrid\", \"nederland\", \"netherlands\"]:\n",
    "            if city in txt:\n",
    "                location_guess = city.title()\n",
    "                break\n",
    "\n",
    "    return {\n",
    "        \"vacancy_id\": str(vac_id),\n",
    "        \"title\": title,\n",
    "        \"location_guess\": location_guess,\n",
    "        \"url\": r.url,  # final url after redirects\n",
    "        \"sections_text\": sections_text,\n",
    "        \"description_text\": description_text,\n",
    "        \"date_taken_utc\": date_taken_utc,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# RUN SCRAPE (ID SCAN)\n",
    "# =========================\n",
    "rows = []\n",
    "misses = 0\n",
    "scanned = 0\n",
    "valid_pages = 0\n",
    "\n",
    "print(f\"\\nScanning De Volksbank job IDs {START_ID}..{END_ID} for Data Analyst / BI Analyst ...\\n\")\n",
    "\n",
    "for vac_id in range(START_ID, END_ID + 1):\n",
    "    scanned += 1\n",
    "    rec = None\n",
    "    try:\n",
    "        rec = fetch_vacancy(vac_id)\n",
    "    except Exception:\n",
    "        rec = None\n",
    "\n",
    "    if rec is None:\n",
    "        misses += 1\n",
    "        if misses % 50 == 0:\n",
    "            print(f\"… up to ID {vac_id} | consecutive misses={misses} | matches={len(rows)}\")\n",
    "        if misses >= MAX_CONSECUTIVE_MISSES:\n",
    "            print(f\"Stopping early: {misses} consecutive misses (likely outside active ID range).\")\n",
    "            break\n",
    "    else:\n",
    "        valid_pages += 1\n",
    "        misses = 0\n",
    "\n",
    "        if is_relevant(rec[\"title\"], rec[\"description_text\"]):\n",
    "            print(f\"✅ MATCH {vac_id}: {rec['title']}\")\n",
    "            rows.append(rec)\n",
    "        else:\n",
    "            # comment out next line if too noisy\n",
    "            print(f\"skip {vac_id}: {rec['title']}\")\n",
    "\n",
    "    time.sleep(SLEEP_SEC)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Exact column order (same as ABN/ING)\n",
    "if not df.empty:\n",
    "    df = df[[\n",
    "        \"vacancy_id\",\n",
    "        \"title\",\n",
    "        \"location_guess\",\n",
    "        \"url\",\n",
    "        \"sections_text\",\n",
    "        \"description_text\",\n",
    "        \"date_taken_utc\",\n",
    "    ]]\n",
    "\n",
    "display(df)\n",
    "print(\"\\nDone.\")\n",
    "print(f\"- Scanned IDs: {scanned}\")\n",
    "print(f\"- Valid pages found: {valid_pages}\")\n",
    "print(f\"- Matches (Data/BI Analyst): {len(df)}\")\n",
    "\n",
    "# =========================\n",
    "# SAVE CSV TO VOLUME\n",
    "# =========================\n",
    "#if df is None or df.empty:\n",
    "#    print(\"No matches found — nothing to save.\")\n",
    "#else:\n",
    "#    try:\n",
    "#        dbutils.fs.mkdirs(VOLUME_DIR)\n",
    "#    except Exception:\n",
    "#        pass\n",
    "\n",
    "#    df.to_csv(OUT_CSV_VOLUME, index=False)\n",
    "#    print(\"✅ Saved CSV to Volume:\", OUT_CSV_VOLUME)\n",
    "#    display(dbutils.fs.ls(VOLUME_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c948c6a4-4a42-4ae3-907d-ca86a6e768d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SAVE CSV TO VOLUME\n",
    "# =========================\n",
    "if df is None or df.empty:\n",
    "    print(\"No matches found — nothing to save.\")\n",
    "else:\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(VOLUME_DIR)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    df.to_csv(OUT_CSV_VOLUME, index=False)\n",
    "    print(\"✅ Saved CSV to Volume:\", OUT_CSV_VOLUME)\n",
    "    display(dbutils.fs.ls(VOLUME_DIR))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_raw_job_posting_data_or_bi_analyst_devolksbank",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
