{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e525d84c-7db5-4c89-826b-32383ad469e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.robotparser\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE = \"https://www.werkenbijabnamro.nl/\"\n",
    "TARGET = \"https://www.werkenbijabnamro.nl/vacatures\"\n",
    "UA = \"MarkusJobScraper/1.0 (+contact: youremail@example.com)\"\n",
    "\n",
    "robots_url = urljoin(BASE, \"robots.txt\")\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(robots_url)\n",
    "rp.read()\n",
    "\n",
    "print(\"robots.txt:\", robots_url)\n",
    "print(\"allowed?\", rp.can_fetch(UA, TARGET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e007ee0-0405-4886-b753-a73ea2fc7bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "\n",
    "START_URL = \"https://www.werkenbijabnamro.nl/vacatures\"\n",
    "KEYWORD = \"data analyst\"          # kamu bisa ganti: \"data\", \"analytics\", dll\n",
    "MAX_PAGES = 50                    # safety\n",
    "DELAY_SEC = 0.8                   # sopan: jeda antar page/detail request\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "    return s\n",
    "\n",
    "def same_domain(url: str, domain: str) -> bool:\n",
    "    try:\n",
    "        return urlparse(url).netloc.endswith(domain)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "async def extract_job_detail(page, url: str) -> dict:\n",
    "    await page.goto(url, wait_until=\"networkidle\", timeout=60000)\n",
    "    await page.wait_for_timeout(int(DELAY_SEC * 1000))\n",
    "\n",
    "    # Title: try h1 first\n",
    "    title = \"\"\n",
    "    try:\n",
    "        h1 = await page.query_selector(\"h1\")\n",
    "        if h1:\n",
    "            title = normalize_space(await h1.inner_text())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Description: prioritize main/article, fallback to body text\n",
    "    desc = \"\"\n",
    "    for sel in [\"main article\", \"main\", \"article\", \"[data-testid*='description']\", \".vacancy\", \".job\", \"body\"]:\n",
    "        try:\n",
    "            el = await page.query_selector(sel)\n",
    "            if el:\n",
    "                txt = normalize_space(await el.inner_text())\n",
    "                # ambil yang “cukup panjang” supaya tidak cuma menu/footer\n",
    "                if len(txt) > len(desc):\n",
    "                    desc = txt\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Optional: try to detect location-like text (best effort)\n",
    "    location = \"\"\n",
    "    try:\n",
    "        # cari elemen yang mengandung kata lokasi umum\n",
    "        candidates = await page.query_selector_all(\"main *\")\n",
    "        for c in candidates[:200]:  # limit biar cepat\n",
    "            t = normalize_space(await c.inner_text())\n",
    "            if any(k in t.lower() for k in [\"amsterdam\", \"utrecht\", \"rotterdam\", \"the hague\", \"den haag\", \"hybrid\", \"netherlands\", \"nederland\"]):\n",
    "                if 5 < len(t) < 120:\n",
    "                    location = t\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"location_guess\": location,\n",
    "        \"description\": desc,\n",
    "    }\n",
    "\n",
    "async def collect_job_links(page) -> set[str]:\n",
    "    # ambil semua link yang mengandung /vacatures/ (umumnya detail page)\n",
    "    links = await page.eval_on_selector_all(\n",
    "        \"a[href]\",\n",
    "        \"\"\"els => els\n",
    "            .map(e => e.href)\n",
    "            .filter(h => h && h.includes('/vacatures/'))\"\"\"\n",
    "    )\n",
    "    return set(links)\n",
    "\n",
    "async def click_next_if_exists(page) -> bool:\n",
    "    # beberapa kemungkinan tombol next\n",
    "    selectors = [\n",
    "        \"a[rel='next']\",\n",
    "        \"a[aria-label*='Next' i]\",\n",
    "        \"button[aria-label*='Next' i]\",\n",
    "        \"a:has-text('Next')\",\n",
    "        \"button:has-text('Next')\",\n",
    "        \"a:has-text('Volgende')\",\n",
    "        \"button:has-text('Volgende')\",\n",
    "        \"a:has-text('>')\",\n",
    "    ]\n",
    "    for sel in selectors:\n",
    "        try:\n",
    "            el = await page.query_selector(sel)\n",
    "            if el:\n",
    "                await el.click()\n",
    "                await page.wait_for_load_state(\"networkidle\")\n",
    "                await page.wait_for_timeout(int(DELAY_SEC * 1000))\n",
    "                return True\n",
    "        except:\n",
    "            continue\n",
    "    return False\n",
    "\n",
    "async def main():\n",
    "    domain = \"werkenbijabnamro.nl\"\n",
    "    keyword = KEYWORD.lower()\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"MarkusJobScraper/1.0 (+contact: youremail@example.com)\"\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "\n",
    "        # 1) Crawl listing pages -> kumpulkan semua link detail\n",
    "        await page.goto(START_URL, wait_until=\"networkidle\", timeout=60000)\n",
    "        await page.wait_for_timeout(int(DELAY_SEC * 1000))\n",
    "\n",
    "        all_links = set()\n",
    "        for i in range(1, MAX_PAGES + 1):\n",
    "            links = await collect_job_links(page)\n",
    "            links = {u for u in links if same_domain(u, domain)}\n",
    "            before = len(all_links)\n",
    "            all_links |= links\n",
    "\n",
    "            print(f\"[Listing page {i}] got {len(links)} links, total unique {len(all_links)}\")\n",
    "\n",
    "            # stop kalau tidak ada tombol next atau tidak ada link baru (heuristic)\n",
    "            clicked = await click_next_if_exists(page)\n",
    "            if not clicked:\n",
    "                break\n",
    "            if len(all_links) == before and i >= 2:\n",
    "                # tidak nambah link; kemungkinan pagination habis / loop\n",
    "                break\n",
    "\n",
    "        # 2) Buka tiap job page -> extract detail & filter keyword\n",
    "        detail_page = await context.new_page()\n",
    "        rows = []\n",
    "        for idx, url in enumerate(sorted(all_links), start=1):\n",
    "            try:\n",
    "                data = await extract_job_detail(detail_page, url)\n",
    "\n",
    "                haystack = f\"{data.get('title','')} {data.get('description','')}\".lower()\n",
    "                if keyword in haystack:\n",
    "                    print(f\"✅ [{idx}/{len(all_links)}] MATCH: {data.get('title','(no title)')}\")\n",
    "                    rows.append(data)\n",
    "                else:\n",
    "                    print(f\"… [{idx}/{len(all_links)}] skip\")\n",
    "            except PlaywrightTimeoutError:\n",
    "                print(f\"⚠️ timeout: {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ error: {url} -> {e}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # 3) Save\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(\"abnamro_data_analyst_jobs.csv\", index=False)\n",
    "    print(\"\\nSaved: abnamro_data_analyst_jobs.csv\")\n",
    "    print(df[[\"title\", \"url\"]].head(20))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c075254a-65de-49b9-894d-c52190b938ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q requests beautifulsoup4 pandas lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6d4a99-c812-40e1-afb1-b8f374cdd6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d461b6-26bf-4c21-b507-ef11a8bafaf4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"title\":311},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1766700000848}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE = \"https://www.werkenbijabnamro.nl\"\n",
    "UA = \"MarkusJobScraper/1.0 (+contact: youremail@example.com)\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": UA,\n",
    "    \"Accept-Language\": \"en-GB,en;q=0.9,nl;q=0.8\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "# >>> TUNING PARAMS <<<\n",
    "START_ID = 8300\n",
    "END_ID   = 9500\n",
    "SLEEP_SEC = 0.6\n",
    "\n",
    "# stop lebih cepat kalau sudah banyak miss beruntun\n",
    "MAX_CONSECUTIVE_MISSES = 250\n",
    "\n",
    "# filter: title/desc harus contain salah satu ini\n",
    "KEYWORDS = [\n",
    "    \"data analyst\",\n",
    "    \"senior data analyst\",\n",
    "    \"analytics analyst\",\n",
    "    \"bi analyst\",\n",
    "    \"reporting analyst\",\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def fetch_vacancy_by_id(vac_id: int) -> dict | None:\n",
    "    \"\"\"\n",
    "    Coba akses /en/vacancy/<id> (tanpa slug).\n",
    "    Umumnya website akan redirect ke /en/vacancy/<id>/<slug>.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE}/en/vacancy/{vac_id}\"\n",
    "    r = session.get(url, allow_redirects=True, timeout=30)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    final_url = r.url\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    h1 = soup.find(\"h1\")\n",
    "    title = norm(h1.get_text(\" \")) if h1 else \"\"\n",
    "\n",
    "    main = soup.find(\"main\") or soup\n",
    "    full_text = norm(main.get_text(\" \"))\n",
    "\n",
    "    # ambil section2 kalau ada (h2/h3)\n",
    "    sections = []\n",
    "    for hdr in main.find_all([\"h2\", \"h3\"]):\n",
    "        header = norm(hdr.get_text(\" \"))\n",
    "        if not header:\n",
    "            continue\n",
    "        parts = []\n",
    "        for sib in hdr.find_all_next():\n",
    "            if sib.name in [\"h2\", \"h3\"]:\n",
    "                break\n",
    "            if sib.name in [\"p\", \"ul\", \"ol\"]:\n",
    "                parts.append(norm(sib.get_text(\" \")))\n",
    "        content = norm(\" \".join([p for p in parts if p]))\n",
    "        if content and len(content) > 80:\n",
    "            sections.append(f\"{header}: {content}\")\n",
    "\n",
    "    return {\n",
    "        \"vacancy_id\": vac_id,\n",
    "        \"url\": final_url,\n",
    "        \"title\": title,\n",
    "        \"sections_text\": \"\\n\\n\".join(sections),\n",
    "        \"description_text\": full_text,\n",
    "    }\n",
    "\n",
    "def is_relevant(rec: dict) -> bool:\n",
    "    hay = f\"{rec.get('title','')} {rec.get('description_text','')}\".lower()\n",
    "    return any(k in hay for k in KEYWORDS)\n",
    "\n",
    "rows = []\n",
    "misses = 0\n",
    "\n",
    "for vac_id in range(START_ID, END_ID + 1):\n",
    "    rec = None\n",
    "    try:\n",
    "        rec = fetch_vacancy_by_id(vac_id)\n",
    "    except Exception as e:\n",
    "        # treat error as miss (network hiccup)\n",
    "        rec = None\n",
    "\n",
    "    if rec is None or not rec.get(\"title\"):\n",
    "        misses += 1\n",
    "        if misses % 50 == 0:\n",
    "            print(f\"… scanned up to {vac_id}, consecutive misses={misses}\")\n",
    "        if misses >= MAX_CONSECUTIVE_MISSES:\n",
    "            print(f\"Stopping early: {misses} consecutive misses (likely passed current ID range).\")\n",
    "            break\n",
    "    else:\n",
    "        misses = 0\n",
    "        if is_relevant(rec):\n",
    "            print(f\"✅ MATCH {vac_id}: {rec['title']}\")\n",
    "            rows.append(rec)\n",
    "        else:\n",
    "            print(f\"skip {vac_id}: {rec['title']}\")\n",
    "\n",
    "    time.sleep(SLEEP_SEC)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df[[\"vacancy_id\",\"title\",\"url\"]])\n",
    "print(f\"Done. Matches: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f896637a-b00d-46bf-bf79-8412f82382f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE = \"https://www.werkenbijabnamro.nl\"\n",
    "UA = \"MarkusJobScraper/1.0 (+contact: youremail@example.com)\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": UA,\n",
    "    \"Accept-Language\": \"en-GB,en;q=0.9,nl;q=0.8\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "# Vacancy IDs (tune for speed; if you don't know, start with 8500–9500)\n",
    "START_ID = 8500\n",
    "END_ID   = 9500\n",
    "\n",
    "SLEEP_SEC = 0.35                 # be polite\n",
    "MAX_CONSECUTIVE_MISSES = 200      # auto-stop if we've likely passed valid range\n",
    "\n",
    "# Filter: Data Analyst / BI Analyst (EN + NL)\n",
    "KEYWORD_PATTERNS = [\n",
    "    r\"\\bdata analyst\\b\",\n",
    "    r\"\\bsenior data analyst\\b\",\n",
    "    r\"\\bbi analyst\\b\",\n",
    "    r\"\\bbusiness intelligence analyst\\b\",\n",
    "    r\"\\bdata analist\\b\",\n",
    "    r\"\\bbi analist\\b\",\n",
    "]\n",
    "\n",
    "# Output paths\n",
    "OUT_CSV_DBFS = \"/dbfs/tmp/abnamro_data_bi_analyst_jobs.csv\"\n",
    "OUT_DELTA_PATH = \"/tmp/abnamro_data_bi_analyst_jobs\"  # DBFS path for Delta\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def is_relevant(title: str, description: str) -> bool:\n",
    "    text = f\"{title} {description}\"\n",
    "    return any(re.search(pat, text, flags=re.I) for pat in KEYWORD_PATTERNS)\n",
    "\n",
    "def fetch_vacancy(vac_id: int) -> dict | None:\n",
    "    \"\"\"\n",
    "    Try /en/vacancy/<id> (often redirects to /en/vacancy/<id>/<slug>)\n",
    "    Return parsed vacancy dict or None if not found.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE}/en/vacancy/{vac_id}\"\n",
    "    r = session.get(url, allow_redirects=True, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    main = soup.find(\"main\") or soup\n",
    "\n",
    "    # title\n",
    "    h1 = soup.find(\"h1\")\n",
    "    title = norm(h1.get_text(\" \")) if h1 else \"\"\n",
    "    if not title:\n",
    "        return None\n",
    "\n",
    "    # full text (for keyword/NLP)\n",
    "    full_text = norm(main.get_text(\" \"))\n",
    "\n",
    "    # structured sections (optional, nice for portfolio)\n",
    "    sections = []\n",
    "    for hdr in main.find_all([\"h2\", \"h3\"]):\n",
    "        header = norm(hdr.get_text(\" \"))\n",
    "        if not header:\n",
    "            continue\n",
    "\n",
    "        parts = []\n",
    "        for sib in hdr.find_all_next():\n",
    "            if sib.name in [\"h2\", \"h3\"]:\n",
    "                break\n",
    "            if sib.name in [\"p\", \"ul\", \"ol\"]:\n",
    "                parts.append(norm(sib.get_text(\" \")))\n",
    "\n",
    "        content = norm(\" \".join([p for p in parts if p]))\n",
    "        if len(content) >= 80:\n",
    "            sections.append(f\"{header}: {content}\")\n",
    "\n",
    "    # lightweight \"location guess\" (best effort)\n",
    "    location_guess = \"\"\n",
    "    top_text = full_text[:1000].lower()\n",
    "    for city in [\"amsterdam\", \"utrecht\", \"rotterdam\", \"the hague\", \"den haag\", \"diemen\", \"amstelveen\", \"netherlands\", \"nederland\", \"hybrid\"]:\n",
    "        if city in top_text:\n",
    "            location_guess = city.title()\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"vacancy_id\": vac_id,\n",
    "        \"url\": r.url,  # final url after redirect\n",
    "        \"title\": title,\n",
    "        \"location_guess\": location_guess,\n",
    "        \"sections_text\": \"\\n\\n\".join(sections),\n",
    "        \"description_text\": full_text,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "rows = []\n",
    "misses = 0\n",
    "scanned = 0\n",
    "found_pages = 0\n",
    "\n",
    "print(f\"Scanning vacancy IDs {START_ID}..{END_ID} for Data Analyst / BI Analyst ...\")\n",
    "\n",
    "for vac_id in range(START_ID, END_ID + 1):\n",
    "    scanned += 1\n",
    "    rec = None\n",
    "    try:\n",
    "        rec = fetch_vacancy(vac_id)\n",
    "    except Exception:\n",
    "        rec = None\n",
    "\n",
    "    if rec is None:\n",
    "        misses += 1\n",
    "        if misses % 50 == 0:\n",
    "            print(f\"… up to ID {vac_id} | consecutive misses={misses} | total matches={len(rows)}\")\n",
    "        if misses >= MAX_CONSECUTIVE_MISSES:\n",
    "            print(f\"Stopping early: {misses} consecutive misses (likely passed active ID range).\")\n",
    "            break\n",
    "    else:\n",
    "        found_pages += 1\n",
    "        misses = 0\n",
    "\n",
    "        if is_relevant(rec[\"title\"], rec[\"description_text\"]):\n",
    "            print(f\"✅ MATCH {vac_id}: {rec['title']}\")\n",
    "            rows.append(rec)\n",
    "        else:\n",
    "            print(f\"skip {vac_id}: {rec['title']}\")\n",
    "\n",
    "    time.sleep(SLEEP_SEC)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Keep nice columns order\n",
    "if not df.empty:\n",
    "    df = df[[\n",
    "        \"vacancy_id\",\n",
    "        \"title\",\n",
    "        \"location_guess\",\n",
    "        \"url\",\n",
    "        \"sections_text\",\n",
    "        \"description_text\",\n",
    "    ]]\n",
    "\n",
    "display(df)\n",
    "print(f\"\\nDone.\")\n",
    "print(f\"- Scanned IDs: {scanned}\")\n",
    "print(f\"- Valid pages found: {found_pages}\")\n",
    "print(f\"- Matches (Data/BI Analyst): {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3388b19-fcf6-428a-912e-f7888c93bc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE = \"https://www.werkenbijabnamro.nl\"\n",
    "UA = \"MarkusJobScraper/1.0 (+contact: youremail@example.com)\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": UA,\n",
    "    \"Accept-Language\": \"en-GB,en;q=0.9,nl;q=0.8\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "# Vacancy ID scan range (tune as needed)\n",
    "START_ID = 8500\n",
    "END_ID   = 9500\n",
    "\n",
    "SLEEP_SEC = 0.35\n",
    "MAX_CONSECUTIVE_MISSES = 200\n",
    "\n",
    "# =========================\n",
    "# FILTER: DATA ENGINEER (EN + NL)\n",
    "# =========================\n",
    "KEYWORD_PATTERNS = [\n",
    "    # core titles\n",
    "    r\"\\bdata engineer\\b\",\n",
    "    r\"\\bsenior data engineer\\b\",\n",
    "    r\"\\blead data engineer\\b\",\n",
    "    r\"\\bprincipal data engineer\\b\",\n",
    "    r\"\\bdata engineering\\b\",\n",
    "    r\"\\bdata platform engineer\\b\",\n",
    "    r\"\\bplatform engineer\\b\",\n",
    "\n",
    "    # NL variants\n",
    "    r\"\\bdata engineer\\b\",            # same in NL\n",
    "    r\"\\bdata engineer(s)?\\b\",\n",
    "    r\"\\bdata[- ]engineer\\b\",\n",
    "\n",
    "    # adjacent titles\n",
    "    r\"\\banalytics engineer\\b\",\n",
    "    r\"\\bdata ops\\b|\\bdataops\\b\",\n",
    "    r\"\\bmlops\\b\",\n",
    "    r\"\\bdevops\\b.*\\bdata\\b\",\n",
    "    r\"\\bcloud engineer\\b.*\\bdata\\b\",\n",
    "\n",
    "    # common stack keywords (to catch postings that don't say \"data engineer\" in title)\n",
    "    r\"\\betl\\b|\\belt\\b\",\n",
    "    r\"\\bdata pipeline(s)?\\b\",\n",
    "    r\"\\bdata warehouse\\b|\\bdwh\\b\",\n",
    "    r\"\\bdelta lake\\b|\\blakehouse\\b\",\n",
    "    r\"\\bspark\\b|\\bpyspark\\b\",\n",
    "    r\"\\bdatabricks\\b\",\n",
    "    r\"\\bkafka\\b\",\n",
    "    r\"\\bairflow\\b\",\n",
    "    r\"\\bdbt\\b\",\n",
    "    r\"\\bazure\\b|\\baws\\b|\\bgcp\\b\",\n",
    "    r\"\\bsynapse\\b|\\bfabric\\b\",\n",
    "    r\"\\bbigquery\\b|\\bsnowflake\\b\",\n",
    "    r\"\\bterraform\\b\",\n",
    "]\n",
    "\n",
    "# Output folder (Unity Catalog Volume)\n",
    "VOLUME_DIR = \"/Volumes/skills_intelligence/00_job_posting_landing_zone/financial_sector/data_engineer\"\n",
    "FILE_DATE = datetime.now(timezone.utc).strftime(\"%Y%m%d\")\n",
    "OUT_CSV_VOLUME = f\"{VOLUME_DIR}/abnamro_data_engineer{FILE_DATE}.csv\"\n",
    "\n",
    "print(\"CSV target:\", OUT_CSV_VOLUME)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def is_relevant(title: str, description: str) -> bool:\n",
    "    text = f\"{title} {description}\"\n",
    "    return any(re.search(pat, text, flags=re.I) for pat in KEYWORD_PATTERNS)\n",
    "\n",
    "def fetch_vacancy(vac_id: int) -> dict | None:\n",
    "    \"\"\"\n",
    "    Try /en/vacancy/<id> (often redirects to /en/vacancy/<id>/<slug>)\n",
    "    Return parsed vacancy dict or None if not found.\n",
    "    Includes per-record date_taken_utc.\n",
    "    \"\"\"\n",
    "    date_taken_utc = datetime.now(timezone.utc)\n",
    "\n",
    "    url = f\"{BASE}/en/vacancy/{vac_id}\"\n",
    "    r = session.get(url, allow_redirects=True, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    main = soup.find(\"main\") or soup\n",
    "\n",
    "    # Title\n",
    "    h1 = soup.find(\"h1\")\n",
    "    title = norm(h1.get_text(\" \")) if h1 else \"\"\n",
    "    if not title:\n",
    "        return None\n",
    "\n",
    "    # Full text\n",
    "    full_text = norm(main.get_text(\" \"))\n",
    "\n",
    "    # Structured sections\n",
    "    sections = []\n",
    "    for hdr in main.find_all([\"h2\", \"h3\"]):\n",
    "        header = norm(hdr.get_text(\" \"))\n",
    "        if not header:\n",
    "            continue\n",
    "\n",
    "        parts = []\n",
    "        for sib in hdr.find_all_next():\n",
    "            if sib.name in [\"h2\", \"h3\"]:\n",
    "                break\n",
    "            if sib.name in [\"p\", \"ul\", \"ol\"]:\n",
    "                parts.append(norm(sib.get_text(\" \")))\n",
    "\n",
    "        content = norm(\" \".join([p for p in parts if p]))\n",
    "        if len(content) >= 80:\n",
    "            sections.append(f\"{header}: {content}\")\n",
    "\n",
    "    # Best-effort location guess\n",
    "    location_guess = \"\"\n",
    "    top_text = full_text[:1200].lower()\n",
    "    for city in [\n",
    "        \"amsterdam\", \"utrecht\", \"rotterdam\", \"the hague\",\n",
    "        \"den haag\", \"diemen\", \"amstelveen\",\n",
    "        \"netherlands\", \"nederland\", \"hybrid\"\n",
    "    ]:\n",
    "        if city in top_text:\n",
    "            location_guess = city.title()\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"vacancy_id\": vac_id,\n",
    "        \"url\": r.url,\n",
    "        \"title\": title,\n",
    "        \"location_guess\": location_guess,\n",
    "        \"sections_text\": \"\\n\\n\".join(sections),\n",
    "        \"description_text\": full_text,\n",
    "        \"date_taken_utc\": date_taken_utc,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# RUN SCRAPE\n",
    "# =========================\n",
    "rows = []\n",
    "misses = 0\n",
    "scanned = 0\n",
    "valid_pages = 0\n",
    "\n",
    "print(f\"\\nScanning vacancy IDs {START_ID}..{END_ID} for Data Engineer ...\\n\")\n",
    "\n",
    "for vac_id in range(START_ID, END_ID + 1):\n",
    "    scanned += 1\n",
    "    rec = None\n",
    "    try:\n",
    "        rec = fetch_vacancy(vac_id)\n",
    "    except Exception:\n",
    "        rec = None\n",
    "\n",
    "    if rec is None:\n",
    "        misses += 1\n",
    "        if misses % 50 == 0:\n",
    "            print(f\"… up to ID {vac_id} | consecutive misses={misses} | matches={len(rows)}\")\n",
    "        if misses >= MAX_CONSECUTIVE_MISSES:\n",
    "            print(f\"Stopping early: {misses} consecutive misses (likely past active ID range).\")\n",
    "            break\n",
    "    else:\n",
    "        valid_pages += 1\n",
    "        misses = 0\n",
    "\n",
    "        if is_relevant(rec[\"title\"], rec[\"description_text\"]):\n",
    "            print(f\"✅ MATCH {vac_id}: {rec['title']}\")\n",
    "            rows.append(rec)\n",
    "        else:\n",
    "            print(f\"skip {vac_id}: {rec['title']}\")\n",
    "\n",
    "    time.sleep(SLEEP_SEC)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "if not df.empty:\n",
    "    df = df[[\n",
    "        \"vacancy_id\",\n",
    "        \"title\",\n",
    "        \"location_guess\",\n",
    "        \"url\",\n",
    "        \"sections_text\",\n",
    "        \"description_text\",\n",
    "        \"date_taken_utc\",\n",
    "    ]]\n",
    "\n",
    "display(df)\n",
    "print(\"\\nDone.\")\n",
    "print(f\"- Scanned IDs: {scanned}\")\n",
    "print(f\"- Valid pages found: {valid_pages}\")\n",
    "print(f\"- Matches (Data Engineer): {len(df)}\")\n",
    "\n",
    "# =========================\n",
    "# SAVE CSV TO VOLUME\n",
    "# =========================\n",
    "if df is None or df.empty:\n",
    "    print(\"No matches found — nothing to save.\")\n",
    "else:\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(VOLUME_DIR)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    df.to_csv(OUT_CSV_VOLUME, index=False)\n",
    "    print(\"✅ Saved CSV to Volume:\", OUT_CSV_VOLUME)\n",
    "    display(dbutils.fs.ls(VOLUME_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99ad2716-2854-46ab-8f45-908d2f9882c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_raw_job_posting_data_engineer_abnamro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
