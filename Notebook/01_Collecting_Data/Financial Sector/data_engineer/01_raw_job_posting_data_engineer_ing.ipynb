{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d7ce75-0d96-4376-90ca-09b2ba3c47ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.robotparser\n",
    "\n",
    "robots_url = \"https://careers.ing.com/robots.txt\"\n",
    "target_url = \"https://careers.ing.com/en/job/\"\n",
    "\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(robots_url)\n",
    "rp.read()\n",
    "\n",
    "print(\"Allowed?\", rp.can_fetch(\"*\", target_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20faf842-1cf5-43db-9ad6-380acae2cc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook — ING Job Scraper (Data Analyst / BI Analyst)\n",
    "# Output schema matches ABN AMRO:\n",
    "# vacancy_id, title, location_guess, url, sections_text, description_text, date_taken_utc\n",
    "# Save to: /Volumes/skills_intelligence/00_job_posting_landing_zone/financial_sector/data_or_bi_analyst/ingYYYYMMDD.csv\n",
    "\n",
    "%pip install -q requests beautifulsoup4 pandas lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c343323-054f-4320-b528-a30425bd5759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08a03e5-dbd8-4c90-a37c-873b3c0bbd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "START_URL = \"https://careers.ing.com/en/search-jobs/Netherlands/2618/2/2750405/52x25/5x75/50/2\"\n",
    "\n",
    "UA = \"MarkusJobScraper/1.0 (+contact: youremail@example.com)\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": UA,\n",
    "    \"Accept-Language\": \"en-GB,en;q=0.9,nl;q=0.8\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "SLEEP_SEC = 0.35\n",
    "MAX_LISTING_PAGES = 40\n",
    "\n",
    "# =========================\n",
    "# FILTER: DATA ENGINEER (EN + NL)\n",
    "# =========================\n",
    "KEYWORD_PATTERNS = [\n",
    "    # core titles\n",
    "    r\"\\bdata engineer\\b\",\n",
    "    r\"\\bsenior data engineer\\b\",\n",
    "    r\"\\blead data engineer\\b\",\n",
    "    r\"\\bprincipal data engineer\\b\",\n",
    "    r\"\\bdata engineering\\b\",\n",
    "    r\"\\banalytics engineer\\b\",\n",
    "    r\"\\bdata platform engineer\\b\",\n",
    "\n",
    "    # NL variants\n",
    "    r\"\\bdata[- ]engineer\\b\",\n",
    "\n",
    "    # stack & responsibilities (ING-style postings)\n",
    "    r\"\\betl\\b|\\belt\\b\",\n",
    "    r\"\\bdata pipeline(s)?\\b\",\n",
    "    r\"\\bdata platform\\b\",\n",
    "    r\"\\blakehouse\\b|\\bdelta lake\\b\",\n",
    "    r\"\\bspark\\b|\\bpyspark\\b\",\n",
    "    r\"\\bdatabricks\\b\",\n",
    "    r\"\\bkafka\\b\",\n",
    "    r\"\\bairflow\\b\",\n",
    "    r\"\\bdbt\\b\",\n",
    "    r\"\\bazure\\b|\\bsynapse\\b|\\bfabric\\b\",\n",
    "    r\"\\bsnowflake\\b|\\bbigquery\\b\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# OUTPUT (Unity Catalog Volume)\n",
    "# =========================\n",
    "VOLUME_DIR = \"/Volumes/skills_intelligence/00_job_posting_landing_zone/financial_sector/data_engineer\"\n",
    "FILE_DATE = datetime.now(timezone.utc).strftime(\"%Y%m%d\")\n",
    "OUT_CSV_VOLUME = f\"{VOLUME_DIR}/ing_data_engineer{FILE_DATE}.csv\"\n",
    "\n",
    "print(\"CSV target:\", OUT_CSV_VOLUME)\n",
    "\n",
    "# =========================\n",
    "# SESSION\n",
    "# =========================\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "def get_html(url: str, timeout=30) -> str:\n",
    "    r = session.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "\n",
    "def is_relevant(title: str, description: str) -> bool:\n",
    "    text = f\"{title} {description}\"\n",
    "    return any(re.search(p, text, flags=re.I) for p in KEYWORD_PATTERNS)\n",
    "\n",
    "def extract_ing_job_id(job_url: str) -> str:\n",
    "    m = re.search(r\"/en/job/(\\d+)\", job_url)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_job_links_and_next(listing_url: str):\n",
    "    html = get_html(listing_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    links = set()\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\") or \"\"\n",
    "        if \"/en/job/\" in href:\n",
    "            links.add(urljoin(listing_url, href))\n",
    "\n",
    "    next_url = None\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        if norm(a.get_text(\" \")).lower() == \"next\":\n",
    "            next_url = urljoin(listing_url, a[\"href\"])\n",
    "            break\n",
    "\n",
    "    return links, next_url\n",
    "\n",
    "def parse_sections(main: BeautifulSoup) -> str:\n",
    "    sections = []\n",
    "    for hdr in main.find_all([\"h2\", \"h3\"]):\n",
    "        header = norm(hdr.get_text(\" \"))\n",
    "        if not header:\n",
    "            continue\n",
    "\n",
    "        parts = []\n",
    "        for sib in hdr.find_all_next():\n",
    "            if sib.name in [\"h2\", \"h3\"]:\n",
    "                break\n",
    "            if sib.name in [\"p\", \"ul\", \"ol\"]:\n",
    "                parts.append(norm(sib.get_text(\" \")))\n",
    "\n",
    "        content = norm(\" \".join(parts))\n",
    "        if len(content) >= 80:\n",
    "            sections.append(f\"{header}: {content}\")\n",
    "\n",
    "    return \"\\n\\n\".join(sections)\n",
    "\n",
    "def parse_location_guess(full_text: str) -> str:\n",
    "    lower = full_text.lower()\n",
    "    for city in [\n",
    "        \"amsterdam\", \"utrecht\", \"rotterdam\",\n",
    "        \"the hague\", \"den haag\", \"eindhoven\",\n",
    "        \"netherlands\", \"nederland\", \"hybrid\"\n",
    "    ]:\n",
    "        if city in lower:\n",
    "            return city.title()\n",
    "    return \"\"\n",
    "\n",
    "def fetch_vacancy(job_url: str) -> dict | None:\n",
    "    date_taken_utc = datetime.now(timezone.utc)\n",
    "\n",
    "    html = get_html(job_url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    h1 = soup.find(\"h1\")\n",
    "    title = norm(h1.get_text(\" \")) if h1 else \"\"\n",
    "    if not title:\n",
    "        return None\n",
    "\n",
    "    main = soup.find(\"main\") or soup\n",
    "    description_text = norm(main.get_text(\" \"))\n",
    "\n",
    "    return {\n",
    "        \"vacancy_id\": extract_ing_job_id(job_url),\n",
    "        \"title\": title,\n",
    "        \"location_guess\": parse_location_guess(description_text),\n",
    "        \"url\": job_url,\n",
    "        \"sections_text\": parse_sections(main),\n",
    "        \"description_text\": description_text,\n",
    "        \"date_taken_utc\": date_taken_utc,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 1) COLLECT JOB URLs\n",
    "# =========================\n",
    "all_job_urls = set()\n",
    "url = START_URL\n",
    "\n",
    "for page_no in range(1, MAX_LISTING_PAGES + 1):\n",
    "    links, next_url = extract_job_links_and_next(url)\n",
    "    all_job_urls |= links\n",
    "\n",
    "    print(f\"[Listing page {page_no}] collected {len(links)} | total {len(all_job_urls)}\")\n",
    "\n",
    "    if not next_url or next_url == url:\n",
    "        break\n",
    "\n",
    "    url = next_url\n",
    "    time.sleep(SLEEP_SEC)\n",
    "\n",
    "print(f\"\\nTotal ING job URLs collected: {len(all_job_urls)}\")\n",
    "\n",
    "# =========================\n",
    "# 2) SCRAPE + FILTER\n",
    "# =========================\n",
    "rows = []\n",
    "for i, job_url in enumerate(sorted(all_job_urls), start=1):\n",
    "    try:\n",
    "        rec = fetch_vacancy(job_url)\n",
    "        if rec and is_relevant(rec[\"title\"], rec[\"description_text\"]):\n",
    "            print(f\"✅ MATCH [{i}/{len(all_job_urls)}] {rec['title']}\")\n",
    "            rows.append(rec)\n",
    "        else:\n",
    "            print(f\"skip  [{i}/{len(all_job_urls)}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ error [{i}] {e}\")\n",
    "\n",
    "    time.sleep(SLEEP_SEC)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "if not df.empty:\n",
    "    df = df[\n",
    "        [\n",
    "            \"vacancy_id\",\n",
    "            \"title\",\n",
    "            \"location_guess\",\n",
    "            \"url\",\n",
    "            \"sections_text\",\n",
    "            \"description_text\",\n",
    "            \"date_taken_utc\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "display(df)\n",
    "print(\"Matches (Data Engineer):\", len(df))\n",
    "\n",
    "# =========================\n",
    "# 3) SAVE CSV\n",
    "# =========================\n",
    "if df.empty:\n",
    "    print(\"No matches found — nothing to save.\")\n",
    "else:\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(VOLUME_DIR)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    df.to_csv(OUT_CSV_VOLUME, index=False)\n",
    "    print(\"✅ Saved CSV to Volume:\", OUT_CSV_VOLUME)\n",
    "    display(dbutils.fs.ls(VOLUME_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "583d2903-98dd-4457-9f6b-b2954e530323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_raw_job_posting_data_engineer_ing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
