{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149475c0-6a83-489e-b1f0-a7fd3154e563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 00. Parameters\n",
    "# ============================================\n",
    "\n",
    "CATALOG = \"skills_intelligence\"\n",
    "BRONZE_SCHEMA = \"01_bronze\"\n",
    "TABLE_NAME = \"livrea_soft_skill\"\n",
    "\n",
    "SOURCE_CSV = \"/Volumes/skills_intelligence/00_livrea_data/soft_skill/livrea_soft_skill.csv\"\n",
    "FULL_TABLE_NAME = f\"{CATALOG}.{BRONZE_SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "print(\"SOURCE_CSV:\", SOURCE_CSV)\n",
    "print(\"TARGET_TABLE:\", FULL_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f44f7d-cd0e-42b1-b587-0f91200205b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 01. Set catalog & schema, create schema if needed\n",
    "# ============================================\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {BRONZE_SCHEMA}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "spark.sql(\"SELECT current_catalog(), current_schema()\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee5223e-b033-43a7-933e-7615a7fa4803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 02. Helpers\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def file_exists(path: str) -> bool:\n",
    "    try:\n",
    "        parent, fname = path.rsplit(\"/\", 1)\n",
    "        return any(x.name.rstrip(\"/\") == fname for x in dbutils.fs.ls(parent))\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac91024-4daf-453c-8938-905364ee95ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 03. Read CSV (semicolon separated)\n",
    "# ============================================\n",
    "\n",
    "df_raw = (\n",
    "    spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"sep\", \";\")            # ðŸ”´ IMPORTANT: semicolon separator\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .option(\"quote\", \"\\\"\")\n",
    "        .option(\"escape\", \"\\\"\")\n",
    "        .load(SOURCE_CSV)\n",
    ")\n",
    "\n",
    "display(df_raw)\n",
    "print(\"Rows:\", df_raw.count())\n",
    "print(\"Columns:\", len(df_raw.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf938f2-059f-4f39-a961-65446ebc769e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 04. Add Bronze metadata\n",
    "# ============================================\n",
    "\n",
    "df_bronze = (\n",
    "    df_raw\n",
    "        .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "        .withColumn(\"_source_file\", F.lit(SOURCE_CSV))\n",
    "        .withColumn(\"_source_filename\", F.element_at(F.split(F.lit(SOURCE_CSV), \"/\"), -1))\n",
    "        .withColumn(\"_source_system\", F.lit(\"livrea\"))\n",
    ")\n",
    "\n",
    "display(df_bronze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ee0c90-9f83-4002-a6f6-07e97bfe7418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 05. Write to Delta (Bronze)\n",
    "# ============================================\n",
    "\n",
    "(\n",
    "    df_bronze.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")                 # master data â†’ overwrite is safe\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(FULL_TABLE_NAME)\n",
    ")\n",
    "\n",
    "print(\"âœ… Successfully written to:\", FULL_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8cb61e0-79d8-4334-9138-534924362650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 06. Validation\n",
    "# ============================================\n",
    "\n",
    "spark.sql(f\"SELECT COUNT(*) AS cnt FROM {FULL_TABLE_NAME}\").show()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {FULL_TABLE_NAME}\n",
    "ORDER BY _ingest_ts DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8708dabb-3003-4a51-bf66-71e1fe6f41ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_ingest_softskill_to_table_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
