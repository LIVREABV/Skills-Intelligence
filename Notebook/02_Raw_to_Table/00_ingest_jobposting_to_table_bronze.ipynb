{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78fedf6-e75a-45e3-a8e9-32491229d01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 00. Parameters\n",
    "# ============================================\n",
    "\n",
    "CATALOG = \"skills_intelligence\"\n",
    "BRONZE_SCHEMA = \"01_bronze\"\n",
    "\n",
    "BASE_VOLUME_PATH = \"/Volumes/skills_intelligence/00_job_posting_landing_zone/financial_sector/data_or_bi_analyst\"\n",
    "\n",
    "INGEST_SPECS = [\n",
    "    {\n",
    "        \"source_csv\": f\"{BASE_VOLUME_PATH}/abnamro20251225.csv\",\n",
    "        \"target_table\": f\"{CATALOG}.{BRONZE_SCHEMA}.financial_sector_data_or_bi_analyst_abnamro\",\n",
    "        \"source_system\": \"abnamro\",\n",
    "    },\n",
    "    {\n",
    "        \"source_csv\": f\"{BASE_VOLUME_PATH}/ing20260106.csv\",\n",
    "        \"target_table\": f\"{CATALOG}.{BRONZE_SCHEMA}.financial_sector_data_or_bi_analyst_ing\",\n",
    "        \"source_system\": \"ing\",\n",
    "    },\n",
    "    {\n",
    "        \"source_csv\": f\"{BASE_VOLUME_PATH}/devolksbank20251227.csv\",\n",
    "        \"target_table\": f\"{CATALOG}.{BRONZE_SCHEMA}.financial_sector_data_or_bi_analyst_devolksbank\",\n",
    "        \"source_system\": \"devolksbank\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Ingest specs:\")\n",
    "for s in INGEST_SPECS:\n",
    "    print(\"-\", s[\"source_csv\"], \"->\", s[\"target_table\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2798acc-e208-43b1-b130-0d376dad42c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# Ingest ALL dated CSVs per bank -> Bronze Delta tables\n",
    "# (date-agnostic; schema-forced to avoid Delta merge errors)\n",
    "# ============================================\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# ============================================\n",
    "# 00. Parameters\n",
    "# ============================================\n",
    "CATALOG = \"skills_intelligence\"\n",
    "BRONZE_SCHEMA = \"01_bronze\"\n",
    "\n",
    "BASE_VOLUME_PATH = \"/Volumes/skills_intelligence/00_job_posting_landing_zone/financial_sector/data_or_bi_analyst\"\n",
    "\n",
    "BANKS = [\"abnamro\", \"ing\", \"devolksbank\"]\n",
    "\n",
    "TARGET_TABLES = {\n",
    "    \"abnamro\": f\"{CATALOG}.{BRONZE_SCHEMA}.financial_sector_data_or_bi_analyst_abnamro\",\n",
    "    \"ing\": f\"{CATALOG}.{BRONZE_SCHEMA}.financial_sector_data_or_bi_analyst_ing\",\n",
    "    \"devolksbank\": f\"{CATALOG}.{BRONZE_SCHEMA}.financial_sector_data_or_bi_analyst_devolksbank\",\n",
    "}\n",
    "\n",
    "# Filenames like: ing20260106.csv\n",
    "DATE_RE = re.compile(r\"^(abnamro|ing|devolksbank)(\\d{8})\\.csv$\", re.I)\n",
    "\n",
    "# Expected raw columns from scrapers (force these as STRING to avoid type drift)\n",
    "RAW_COLS = [\n",
    "    \"vacancy_id\",\n",
    "    \"title\",\n",
    "    \"location_guess\",\n",
    "    \"url\",\n",
    "    \"sections_text\",\n",
    "    \"description_text\",\n",
    "    \"date_taken_utc\",\n",
    "]\n",
    "\n",
    "CSV_SCHEMA = StructType([StructField(c, StringType(), True) for c in RAW_COLS])\n",
    "\n",
    "# Metadata columns we add\n",
    "META_COLS = [\"source_system\", \"snapshot_date\", \"source_file\", \"ingested_at_utc\"]\n",
    "\n",
    "# Final column order (raw + meta)\n",
    "FINAL_COLS = RAW_COLS + META_COLS\n",
    "\n",
    "print(\"Base volume path:\", BASE_VOLUME_PATH)\n",
    "\n",
    "# ============================================\n",
    "# 01. Ensure catalog/schema exist\n",
    "# ============================================\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}\")\n",
    "\n",
    "# ============================================\n",
    "# 02. Discover all matching CSVs\n",
    "# ============================================\n",
    "files = dbutils.fs.ls(BASE_VOLUME_PATH)\n",
    "\n",
    "bank_to_files = {b: [] for b in BANKS}\n",
    "for f in files:\n",
    "    name = f.name\n",
    "    m = DATE_RE.match(name)\n",
    "    if not m:\n",
    "        continue\n",
    "    bank = m.group(1).lower()\n",
    "    yyyymmdd = m.group(2)\n",
    "    bank_to_files[bank].append((yyyymmdd, f.path))\n",
    "\n",
    "print(\"\\nDiscovered files:\")\n",
    "for b in BANKS:\n",
    "    items = sorted(bank_to_files[b])\n",
    "    print(f\"- {b}: {len(items)} file(s)\")\n",
    "    for d, p in items[:20]:\n",
    "        print(f\"    {d} -> {p}\")\n",
    "    if len(items) > 20:\n",
    "        print(\"    ...\")\n",
    "\n",
    "# Build ingest specs for ALL snapshots (date-agnostic)\n",
    "INGEST_SPECS = []\n",
    "for bank in BANKS:\n",
    "    for yyyymmdd, path in sorted(bank_to_files[bank]):\n",
    "        INGEST_SPECS.append(\n",
    "            {\n",
    "                \"source_csv\": path,\n",
    "                \"target_table\": TARGET_TABLES[bank],\n",
    "                \"source_system\": bank,\n",
    "                \"snapshot_date\": yyyymmdd,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"\\nTotal ingest specs:\", len(INGEST_SPECS))\n",
    "for s in INGEST_SPECS[:20]:\n",
    "    print(\"-\", s[\"source_csv\"], \"->\", s[\"target_table\"])\n",
    "if len(INGEST_SPECS) > 20:\n",
    "    print(\"...\")\n",
    "\n",
    "# ============================================\n",
    "# 03. Optional: reset tables (DROP) if you want a clean rebuild\n",
    "#     Set RESET_TABLES=True to drop & rebuild from CSVs (prevents legacy type conflicts)\n",
    "# ============================================\n",
    "RESET_TABLES = False  # <-- change to True if you previously created tables with wrong types\n",
    "\n",
    "if RESET_TABLES:\n",
    "    for bank in BANKS:\n",
    "        tgt = TARGET_TABLES[bank]\n",
    "        print(f\"Dropping table (if exists): {tgt}\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {tgt}\")\n",
    "\n",
    "# ============================================\n",
    "# 04. Ingest loop (append) with forced schema\n",
    "# ============================================\n",
    "ingested_at_utc = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "for spec in INGEST_SPECS:\n",
    "    src = spec[\"source_csv\"]\n",
    "    tgt = spec[\"target_table\"]\n",
    "    bank = spec[\"source_system\"]\n",
    "    snap = spec[\"snapshot_date\"]\n",
    "\n",
    "    print(f\"\\nIngesting {bank} snapshot {snap} -> {tgt}\")\n",
    "    print(f\"  source: {src}\")\n",
    "\n",
    "    # Read CSV with fixed schema (all strings) to prevent Delta type conflicts\n",
    "    df_raw = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .schema(CSV_SCHEMA)               # ✅ forced schema (all STRING)\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .option(\"escape\", \"\\\"\")\n",
    "        .option(\"quote\", \"\\\"\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .load(src)\n",
    "    )\n",
    "\n",
    "    # Ensure all expected cols exist (if missing, add as null)\n",
    "    for c in RAW_COLS:\n",
    "        if c not in df_raw.columns:\n",
    "            df_raw = df_raw.withColumn(c, F.lit(None).cast(\"string\"))\n",
    "\n",
    "    # Keep exact raw col order + cast safety\n",
    "    df_raw = df_raw.select([F.col(c).cast(\"string\").alias(c) for c in RAW_COLS])\n",
    "\n",
    "    # Add metadata\n",
    "    df = (\n",
    "        df_raw\n",
    "        .withColumn(\"source_system\", F.lit(bank))\n",
    "        .withColumn(\"snapshot_date\", F.lit(snap))\n",
    "        .withColumn(\"source_file\", F.lit(src))\n",
    "        .withColumn(\"ingested_at_utc\", F.lit(ingested_at_utc))\n",
    "        .select(*FINAL_COLS)  # enforce final col order\n",
    "    )\n",
    "\n",
    "    # Append into Delta table\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(tgt)\n",
    "\n",
    "print(\"\\n✅ Done ingesting ALL snapshots.\")\n",
    "\n",
    "# ============================================\n",
    "# 05. Quick sanity checks\n",
    "# ============================================\n",
    "for bank in BANKS:\n",
    "    tgt = TARGET_TABLES[bank]\n",
    "    try:\n",
    "        cnt = spark.table(tgt).count()\n",
    "        print(f\"- {tgt} rowcount = {cnt}\")\n",
    "    except Exception as e:\n",
    "        print(f\"- {tgt} not readable yet -> {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513898e2-474f-4d12-8fa5-70f7dd28336d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 01. Set catalog & schema, create schema if needed\n",
    "# ============================================\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {BRONZE_SCHEMA}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "spark.sql(\"SELECT current_catalog(), current_schema()\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47737404-97e5-410f-bd86-2bec9fa990bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 02. Helpers: file exists + read csv + add bronze metadata\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def file_exists(path: str) -> bool:\n",
    "    \"\"\"Checks file existence for Volumes/DBFS paths by listing parent dir.\"\"\"\n",
    "    try:\n",
    "        parent, fname = path.rsplit(\"/\", 1)\n",
    "        return any(x.name.rstrip(\"/\") == fname for x in dbutils.fs.ls(parent))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def read_csv_with_metadata(source_csv: str, source_system: str):\n",
    "    df = (\n",
    "        spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"multiLine\", \"true\")\n",
    "            .option(\"escape\", \"\\\"\")\n",
    "            .option(\"quote\", \"\\\"\")\n",
    "            .load(source_csv)\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df.withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "          .withColumn(\"_source_file\", F.lit(source_csv))\n",
    "          .withColumn(\"_source_filename\", F.element_at(F.split(F.lit(source_csv), \"/\"), -1))\n",
    "          .withColumn(\"_source_system\", F.lit(source_system))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7950a885-355c-4bd5-963b-2d446002fd16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 03. Ingest loop: write each csv into its target Delta table\n",
    "# ============================================\n",
    "\n",
    "results = []\n",
    "\n",
    "for spec in INGEST_SPECS:\n",
    "    src = spec[\"source_csv\"]\n",
    "    tgt = spec[\"target_table\"]\n",
    "    sys = spec[\"source_system\"]\n",
    "\n",
    "    if not file_exists(src):\n",
    "        results.append((src, tgt, \"SKIPPED - file not found\", 0))\n",
    "        print(f\"⚠️ SKIP (not found): {src}\")\n",
    "        continue\n",
    "\n",
    "    df_bronze = read_csv_with_metadata(src, sys)\n",
    "    row_count = df_bronze.count()\n",
    "\n",
    "    (df_bronze.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")                # change to \"overwrite\" if you want replace each run\n",
    "        .option(\"mergeSchema\", \"true\")  # schema evolution\n",
    "        .saveAsTable(tgt)\n",
    "    )\n",
    "\n",
    "    results.append((src, tgt, \"OK\", row_count))\n",
    "    print(f\"✅ OK: {src} -> {tgt} (rows: {row_count})\")\n",
    "\n",
    "display(spark.createDataFrame(results, [\"source_csv\", \"target_table\", \"status\", \"rows_written\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce712b8c-f731-49e4-b55d-e2e728c5703b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================\n",
    "# 04. Quick validation: counts & latest ingests\n",
    "# ============================================\n",
    "\n",
    "for spec in INGEST_SPECS:\n",
    "    tgt = spec[\"target_table\"]\n",
    "    try:\n",
    "        spark.sql(f\"SELECT '{tgt}' AS table_name, COUNT(*) AS cnt FROM {tgt}\").show(truncate=False)\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT _source_filename, _source_system, _ingest_ts\n",
    "            FROM {tgt}\n",
    "            ORDER BY _ingest_ts DESC\n",
    "            LIMIT 5\n",
    "        \"\"\").show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not query {tgt}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "107aa262-a0dc-4ae4-ba2f-1d7060c4d5dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_ingest_jobposting_to_table_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
