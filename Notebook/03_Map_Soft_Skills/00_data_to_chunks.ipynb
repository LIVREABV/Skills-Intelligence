{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4559ca2-5779-46ee-9d17-b09518c7901e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Cleaning + Extract core JD\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "END_MARKERS = [\n",
    "    r\"\\babout our internships\\b\",\n",
    "    r\"\\babout ing\\b\",\n",
    "    r\"\\brelated content\\b\",\n",
    "    r\"\\bcookie statement\\b\",\n",
    "    r\"\\bprivacy\\b\",\n",
    "    r\"\\bdiversity\\b\",\n",
    "    r\"\\bequal opportunity\\b\",\n",
    "    r\"\\bload more\\b\",\n",
    "    r\"\\bsign up\\b\",\n",
    "    r\"\\bemail address\\b\",\n",
    "    r\"\\bshare this job\\b\",\n",
    "    r\"\\bsaved jobs\\b\",\n",
    "    r\"\\bexplore the area\\b\",\n",
    "    r\"\\bview all of our available opportunities\\b\",\n",
    "]\n",
    "\n",
    "# Phrase yang biasanya bukan inti JD (hapus di dalam teks, bukan buang satu line)\n",
    "NOISE_PHRASES = [\n",
    "    r\"\\bapply now\\b\",\n",
    "    r\"\\bsave for later\\b\",\n",
    "    r\"\\bscroll down\\b\",\n",
    "    r\"\\bshare this job\\b\",\n",
    "    r\"\\bsaved jobs\\b\",\n",
    "    r\"\\byour place of work\\b\",\n",
    "    r\"\\bmore for you\\b\",\n",
    "    r\"\\bopen in gmail\\b\",\n",
    "    r\"\\blinkedin\\b\",\n",
    "]\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = \"\" if t is None else str(t)\n",
    "    # normalize whitespace (works even if it's one long line)\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    return t.strip()\n",
    "\n",
    "def remove_noise_phrases(text: str) -> str:\n",
    "    t = text\n",
    "    for p in NOISE_PHRASES:\n",
    "        t = re.sub(p, \" \", t, flags=re.IGNORECASE)\n",
    "    # cleanup extra spaces/newlines after removals\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def find_end_after_min(text_lc: str, patterns, min_pos: int) -> Optional[int]:\n",
    "    hits = []\n",
    "    for p in patterns:\n",
    "        for m in re.finditer(p, text_lc, flags=re.IGNORECASE):\n",
    "            if m.start() >= min_pos:\n",
    "                hits.append(m.start())\n",
    "                break\n",
    "    return min(hits) if hits else None\n",
    "\n",
    "def extract_core_jd(\n",
    "    text: str,\n",
    "    min_end_pos: int = 800,     # abaikan END_MARKER yang muncul terlalu awal (menu/header)\n",
    "    min_core_chars: int = 400,  # kalau hasil terlalu pendek, jangan dipotong\n",
    "    max_chars: int = 12000\n",
    ") -> str:\n",
    "    t = normalize_text(text)\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    # penting: hapus noise phrase tanpa membuang seluruh baris\n",
    "    t = remove_noise_phrases(t)\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    t_lc = t.lower()\n",
    "\n",
    "    # selalu start dari awal\n",
    "    end = find_end_after_min(t_lc, END_MARKERS, min_pos=min_end_pos)\n",
    "\n",
    "    core = t[:end].strip() if end else t.strip()\n",
    "\n",
    "    # fallback: kalau kepotong jadi terlalu pendek, anggap END_MARKER false positive\n",
    "    if len(core) < min_core_chars:\n",
    "        core = t.strip()\n",
    "\n",
    "    if len(core) > max_chars:\n",
    "        core = core[:max_chars]\n",
    "\n",
    "    return core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbdac9f-f099-4238-8b30-defaa236383e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ambil 1 row yang kamu pakai tadi\n",
    "row = (\n",
    "    spark.table(\"skills_intelligence.02_silver.financial_sector_data_or_bi_analyst\")\n",
    "    .select(\"Company\",\"description_text\")\n",
    "    .where(\"description_text is not null\")\n",
    "#    .limit(1)\n",
    "    .limit(3)\n",
    "#    .collect()[0]\n",
    "    .collect()[2]\n",
    ")\n",
    "\n",
    "raw = row[\"description_text\"]\n",
    "core = extract_core_jd(raw)\n",
    "\n",
    "print(\"RAW length:\", len(raw))\n",
    "print(\"CORE length:\", len(core))\n",
    "print(\"\\n--- CORE preview ---\\n\")\n",
    "print(core[:6000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c05a23-6fd9-41f9-a95c-c3b752b68e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1 Imports + fungsi cleaning + chunking\n",
    "import re\n",
    "from typing import Optional, List\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# ----------------------------\n",
    "# CLEANING (tanpa START_MARKERS)\n",
    "# ----------------------------\n",
    "END_MARKERS = [\n",
    "    r\"\\babout our internships\\b\",\n",
    "    r\"\\babout ing\\b\",\n",
    "    r\"\\brelated content\\b\",\n",
    "    r\"\\bcookie statement\\b\",\n",
    "    r\"\\bprivacy\\b\",\n",
    "    r\"\\bdiversity\\b\",\n",
    "    r\"\\bequal opportunity\\b\",\n",
    "    r\"\\bload more\\b\",\n",
    "    r\"\\bsign up\\b\",\n",
    "    r\"\\bemail address\\b\",\n",
    "    r\"\\bshare this job\\b\",\n",
    "    r\"\\bsaved jobs\\b\",\n",
    "    r\"\\bexplore the area\\b\",\n",
    "    r\"\\bview all of our available opportunities\\b\",\n",
    "]\n",
    "\n",
    "NOISE_PHRASES = [\n",
    "    r\"\\bapply now\\b\",\n",
    "    r\"\\bsave for later\\b\",\n",
    "    r\"\\bscroll down\\b\",\n",
    "    r\"\\bshare this job\\b\",\n",
    "    r\"\\bsaved jobs\\b\",\n",
    "    r\"\\byour place of work\\b\",\n",
    "    r\"\\bmore for you\\b\",\n",
    "    r\"\\blinkedin\\b\",\n",
    "]\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = \"\" if t is None else str(t)\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    return t.strip()\n",
    "\n",
    "def remove_noise_phrases(text: str) -> str:\n",
    "    t = text\n",
    "    for p in NOISE_PHRASES:\n",
    "        t = re.sub(p, \" \", t, flags=re.IGNORECASE)\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def find_end_after_min(text_lc: str, patterns, min_pos: int) -> Optional[int]:\n",
    "    hits = []\n",
    "    for p in patterns:\n",
    "        for m in re.finditer(p, text_lc, flags=re.IGNORECASE):\n",
    "            if m.start() >= min_pos:\n",
    "                hits.append(m.start())\n",
    "                break\n",
    "    return min(hits) if hits else None\n",
    "\n",
    "def extract_core_jd(\n",
    "    text: str,\n",
    "    min_end_pos: int = 800,\n",
    "    min_core_chars: int = 400,\n",
    "    max_chars: int = 12000\n",
    ") -> str:\n",
    "    t = normalize_text(text)\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    t = remove_noise_phrases(t)\n",
    "    if not t:\n",
    "        return \"\"\n",
    "\n",
    "    end = find_end_after_min(t.lower(), END_MARKERS, min_pos=min_end_pos)\n",
    "    core = t[:end].strip() if end else t.strip()\n",
    "\n",
    "    # fallback kalau kepotong terlalu pendek (marker di header)\n",
    "    if len(core) < min_core_chars:\n",
    "        core = t.strip()\n",
    "\n",
    "    if len(core) > max_chars:\n",
    "        core = core[:max_chars]\n",
    "\n",
    "    return core\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CHUNKING (per-row)\n",
    "# ----------------------------\n",
    "def _split_blocks(text: str) -> List[str]:\n",
    "    text = normalize_text(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "    blocks = []\n",
    "    bullet_pat = re.compile(r\"^(\\s*[-•*]\\s+|\\s*\\d+[\\.\\)]\\s+)\")\n",
    "\n",
    "    for p in paras:\n",
    "        lines = [ln.strip() for ln in p.split(\"\\n\") if ln.strip()]\n",
    "        if not lines:\n",
    "            continue\n",
    "\n",
    "        current = []\n",
    "        in_bullets = bullet_pat.match(lines[0]) is not None\n",
    "\n",
    "        for ln in lines:\n",
    "            is_bullet = bullet_pat.match(ln) is not None\n",
    "            if current and (is_bullet != in_bullets):\n",
    "                blocks.append(\" \".join(current).strip())\n",
    "                current = []\n",
    "                in_bullets = is_bullet\n",
    "            current.append(ln)\n",
    "\n",
    "        if current:\n",
    "            blocks.append(\" \".join(current).strip())\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def chunk_job_description(\n",
    "    text: str,\n",
    "    chunk_size: int = 1500,\n",
    "    overlap: int = 200,\n",
    "    min_chunk_chars: int = 300\n",
    ") -> List[str]:\n",
    "    blocks = _split_blocks(text)\n",
    "    if not blocks:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    def flush(cur: str):\n",
    "        cur = cur.strip()\n",
    "        if len(cur) >= min_chunk_chars:\n",
    "            chunks.append(cur)\n",
    "\n",
    "    for b in blocks:\n",
    "        if not current:\n",
    "            current = b\n",
    "            continue\n",
    "\n",
    "        if len(current) + 1 + len(b) <= chunk_size:\n",
    "            current = current + \" \" + b\n",
    "        else:\n",
    "            flush(current)\n",
    "            tail = current[-overlap:] if overlap > 0 else \"\"\n",
    "            current = (tail + \" \" + b).strip()\n",
    "\n",
    "    flush(current)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Spark UDF wrappers\n",
    "# ----------------------------\n",
    "extract_core_udf = F.udf(lambda x: extract_core_jd(x), StringType())\n",
    "chunk_udf = F.udf(lambda x: chunk_job_description(x, chunk_size=1500, overlap=200, min_chunk_chars=300), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0cbc66a-c401-4c17-8f99-e2a6f19b9713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2 Load table → Clean semua row\n",
    "\n",
    "src = \"skills_intelligence.02_silver.financial_sector_data_or_bi_analyst\"\n",
    "\n",
    "df = (\n",
    "    spark.table(src)\n",
    "    .select(\"Company\", \"description_text\")\n",
    "    .where(F.col(\"description_text\").isNotNull())\n",
    "    .withColumn(\"core_description_text\", extract_core_udf(F.col(\"description_text\")))\n",
    ")\n",
    "\n",
    "# Optional: drop core yang terlalu pendek (noise)\n",
    "df_clean = df.where(F.length(\"core_description_text\") >= 400)\n",
    "\n",
    "display(df_clean.select(\"Company\", F.length(\"description_text\").alias(\"raw_len\"), F.length(\"core_description_text\").alias(\"core_len\")).limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee81a24-657e-4d32-aa31-6d10fac6bf0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3 Chunk per row → explode jadi baris per chunk\n",
    "df_chunks = (\n",
    "    df_clean\n",
    "    .withColumn(\"chunks\", chunk_udf(F.col(\"core_description_text\")))\n",
    "    .withColumn(\"chunk_count\", F.size(\"chunks\"))\n",
    "    .withColumn(\"chunk\", F.explode(\"chunks\"))\n",
    "    .withColumn(\"chunk_len\", F.length(\"chunk\"))\n",
    "    .drop(\"chunks\")  # biar hemat storage\n",
    ")\n",
    "\n",
    "display(df_chunks.select(\"Company\", \"chunk_count\", \"chunk_len\", \"chunk\").limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f868da-763f-46b9-9adb-09c8705927d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4 Tambah metadata + chunk_id (stabil)\n",
    "df_chunks2 = (\n",
    "    df_chunks\n",
    "    .withColumn(\"row_hash\", F.sha2(F.concat_ws(\"||\", F.col(\"Company\"), F.col(\"core_description_text\")), 256))\n",
    "    .withColumn(\"chunk_hash\", F.sha2(F.col(\"chunk\"), 256))\n",
    "    .withColumn(\"created_at\", F.current_timestamp())\n",
    "    .select(\n",
    "        \"Company\",\n",
    "        \"row_hash\",\n",
    "        \"chunk_hash\",\n",
    "        \"chunk\",\n",
    "        \"chunk_len\",\n",
    "        \"chunk_count\",\n",
    "        \"created_at\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_chunks2.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d82b86-b966-4cfb-bced-c991c9cbb0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5 Simpan hasil chunks ke table\n",
    "target = \"skills_intelligence.02_silver.financial_sector_data_or_bi_analyst_core_chunks\"\n",
    "\n",
    "(\n",
    "    df_chunks2\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(target)\n",
    ")\n",
    "\n",
    "print(\"Saved to:\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8951656b-9182-41c9-8355-65b006b83ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6 Quality check cepat\n",
    "qc = (\n",
    "    df_chunks2.groupBy(\"Company\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_chunks\"),\n",
    "        F.avg(\"chunk_len\").alias(\"avg_chunk_len\"),\n",
    "        F.countDistinct(\"row_hash\").alias(\"distinct_rows\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_chunks\"))\n",
    ")\n",
    "\n",
    "display(qc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26139aad-690b-4286-9d6f-c725422f3240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_data_to_chunks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
